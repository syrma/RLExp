# -*- coding: utf-8 -*-
"""PPO_VECENV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/124Q1tqGG4TB3-MyOksAVY_qBlr8ycBny
"""

#!pip install tensorflow
#!pip install pybullet
#!pip install gym_vecenv

import tensorflow as tf
import gym
import pybullet_envs
import time
import math
import gym_vecenv
import tensorflow_probability as tfp

tfd = tfp.distributions

@tf.function
def action(model, obs, env):
    est = model(obs)
    if env.action_space.shape:
        dist = tfd.MultivariateNormalDiag(est, tf.exp(model.log_std))
    else:
        dist = tfd.Categorical(logits=est, dtype=env.action_space.dtype)

    action = dist.sample()
    logprob = tf.reduce_sum(dist.log_prob(action))

    return action, logprob


size = 5000
epochs = 100
opt = tf.optimizers.Adam(learning_rate=1e-2)
γ = 0.99
λ = 0.97
num_env = 5

env_name = "CartPole-v0"

env = gym_vecenv.DummyVecEnv([lambda: gym.make(env_name)] * num_env)
# policy/actor model
model = tf.keras.models.Sequential(
    [
        tf.keras.layers.Dense(
            64, activation="tanh", input_shape=env.observation_space.shape
        ),
        tf.keras.layers.Dense(64, activation="tanh"),
        tf.keras.layers.Dense(
            env.action_space.shape[0] if env.action_space.shape else env.action_space.n
        ),
    ]
)
if env.action_space.shape:
    model.log_std = tf.Variable(tf.fill(env.action_space.shape, -0.5))
model.summary()

# value/critic model
value_model = tf.keras.models.Sequential(
    [
        tf.keras.layers.Dense(
            64, activation="tanh", input_shape=env.observation_space.shape
        ),
        tf.keras.layers.Dense(64, activation="tanh"),
        tf.keras.layers.Dense(1),
    ]
)
value_model.compile("adam", loss="MSE")
value_model.summary()


def loss(obs, act, adv, logprob):
    eps = 0.1

    if act_spc.shape:
        dist = tfd.MultivariateNormalDiag(model(obs), tf.exp(model.log_std))
    else:
        dist = tfd.Categorical(logits=model(obs))

    new_logprob = tf.reduce_sum(dist.log_prob(act), axis=1)

    masks = [tf.cast(adv[i] >= 0, tf.float32) for i in range(num_env)]

    tf.math.multiply(masks, eps)

    epsilon_clip = [
        masks[i] * (1 + eps) + (1 - masks[i]) * (1 - eps) for i in range(num_env)
    ]

    ratio = tf.exp(new_logprob - logprob)

    s1 = tf.math.multiply(ratio, adv)
    s2 = tf.math.multiply(epsilon_clip, adv)

    return -tf.reduce_mean(tf.minimum(s1, s2))

#def loss():
#    eps = 0.1
#    obs, act, adv, logprob = obs_buf, act_buf, gae, prob_buf

#    if act_spc.shape:
#        dist = tfd.MultivariateNormalDiag(model(obs), tf.exp(model.log_std))
#    else:
#        dist = tfd.Categorical(logits=model(obs))
#    new_logprob = dist.log_prob(act)


#    mask = tf.cast(adv >= 0, tf.float32)
#    epsilon_clip = mask * (1 + eps) + (1 - mask) * (1 - eps)
#    ratio = tf.exp(new_logprob - logprob)

#    return -tf.reduce_mean(tf.minimum(ratio * adv, epsilon_clip * adv))

def run_env(env, size, model, value_model, γ, λ):

    obs_buf = tf.TensorArray(obs_spc.dtype, size)
    act_buf = tf.TensorArray(act_spc.dtype, size)
    rew_buf = tf.TensorArray(tf.float32, size)
    prob_buf = tf.TensorArray(tf.float32, size)
    done_buf = tf.TensorArray(tf.bool, size)

    obs = env.reset()
    obs = tf.cast(obs, obs_spc.dtype)

    for i in range(size):
        act, prob = action(model, obs, env)
        new_obs, rew, done, _ = env.step(act.numpy())

        obs_buf = obs_buf.write(i, obs)
        act_buf = act_buf.write(i, act)
        rew_buf = rew_buf.write(i, rew)
        prob_buf = prob_buf.write(i, prob)
        done_buf = done_buf.write(i, done)

        obs = tf.cast(new_obs, obs_spc.dtype)

    obs_buf = obs_buf.stack()
    act_buf = act_buf.stack()
    rew_buf = rew_buf.stack()
    prob_buf = prob_buf.stack()
    done_buf = done_buf.stack()

    # last_val is 0 when done
    last_val = tf.where(done_buf[-1], 0, tf.squeeze(value_model(obs)))

    return obs_buf, act_buf, rew_buf, prob_buf, done_buf, last_val


#@tf.function
def critic(obs_buf, rew_buf, done_buf, last_val, γ, λ):
    rets = []
    # lens = []

    # TODO: turn into a list of tensor arrays
    v_hats = [tf.TensorArray(tf.float32, size) for _ in range(num_env)]
    gae = [tf.TensorArray(tf.float32, size) for _ in range(num_env)]

    # TODO: changer la boucle et remplacer cumprod/cumsum
    last_idx = [0] * num_env

    for i in range(size):
        for j in range(num_env):  # num_env = ?
            if i != size - 1 and not done_buf[i, j]:
                continue

            # sum of discounted rewards
            current_episode = slice(last_idx[j], i + 1)
            ep_idx = range(last_idx[j], i + 1)
            ep_rew = rew_buf[current_episode, j]
            # TODO: le premier argument de tf.fill n'est pas connu à l'avance (ne marche pas avec tf.function)
            discounts = tf.math.cumprod(tf.fill(ep_rew.shape, γ), exclusive=True)
            ep_v_hats = tf.math.cumsum(discounts * ep_rew, reverse=True)
            v_hats[j] = v_hats[j].scatter(ep_idx, ep_v_hats)

            Vs = tf.squeeze(value_model(obs_buf[current_episode, j]), axis=1)
            if i == size - 1:
                Vsp1 = tf.concat([Vs[1:], [last_val[j]]], axis=0)
            else:
                Vsp1 = tf.concat([Vs[1:], [0]], axis=0)

            deltas = rew_buf[current_episode, j] + γ * Vsp1 - Vs

            # compute the advantage function (gae)
            discounts = tf.math.cumprod(tf.fill(deltas.shape, γ * λ), exclusive=True)
            ep_gae = tf.math.cumsum(discounts * deltas, reverse=True)
            gae[j] = gae[j].scatter(ep_idx, ep_gae)

            if i == size - 1:
                rets.append(tf.reduce_sum(rew_buf[current_episode, j]) + last_val[j])
            else:
                rets.append(tf.reduce_sum(rew_buf[current_episode, j]))
            last_idx[j] = i + 1

    v_hats = [v_hat.stack() for v_hat in v_hats]
    gae = [g.stack() for g in gae]

    return rets, v_hats, gae

# training loop
obs_spc = env.observation_space
act_spc = env.action_space
var_list = list(model.trainable_weights)
if env.action_space.shape:
    var_list.append(model.log_std)

for i in range(epochs):
    start_time = time.time()

    obs_buf, act_buf, rew_buf, prob_buf, done_buf, last_val = run_env(env, size, model, value_model, γ, λ)

    end_run_time = time.time()

    rets, v_hats, adv = critic(obs_buf, rew_buf, done_buf, last_val, γ, λ)

    train_start_time = time.time()

    opt.minimize(lambda: loss(obs_buf, act_buf, adv, prob_buf), var_list=var_list)
    hist = value_model.fit(obs_buf, v_hats, batch_size=32)

    run_time = end_run_time - start_time
    critic_time = train_start_time - end_run_time
    train_time = time.time() - train_start_time

    print("run time", run_time, "critic time", critic_time, "train time", train_time)
    #print(f"run time: {run_time}, critic time: {critic_time}, train time: {train_time}")
    print("AvgEpRet:", tf.reduce_mean(rets).numpy())
    if tf.reduce_mean(rets).numpy() > 200:
        print("EpRets:", rets)
        break

